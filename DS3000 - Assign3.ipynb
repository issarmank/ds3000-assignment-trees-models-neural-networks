{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zQzKLj9kaQV"
   },
   "source": [
    "# Grade: 100 points\n",
    "\n",
    "# Assignment 03: Classification with Decision Trees, Ensemble Models and Neural Networks\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Just like assignment 2, you will train various machine learning models for the binary classification task provided in the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition. Note: we will play loosely with the terms validation and test sets and may even use them interchangeably. This Kaggle competition has its own Test set file that you can make predications on and submit for scoring, so for us our \"test\" set could be viewed as a validation set.\n",
    "\n",
    "The task is to predict whether an individual would survive the famous disaster. There are 11 input variables, and 2 output labels: **survived** or **did not survive**.\n",
    "\n",
    "#### **Follow these steps before submitting your assignment:**\n",
    "\n",
    "1. Complete the notebook.\n",
    "\n",
    "2. Make sure all plots have axis labels.\n",
    "\n",
    "3. Once the notebook is complete, `Restart` your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
    "\n",
    "4. Fix any errors until your notebook runs without any problems.\n",
    "\n",
    "5. Please note, a random seed of 42 needs to be set to ensure the reproducability of the results -- *DO NOT* change this random seed. **If you call additional functions that are based on random number generators, you will need to define their seed to 42 as well**.\n",
    "\n",
    "6. Make sure to reference all external code and documentation used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB4v3lrfkaQY"
   },
   "source": [
    "*Note: you may need to install XGBoost. You can do this by running the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLTagMVCkaQY"
   },
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgKa9FwakaQZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVxMl76-kaQZ"
   },
   "source": [
    "# Q1 - Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UxHow8ykaQZ"
   },
   "source": [
    "`https://www.kaggle.com/competitions/titanic` contains data on passengers of the famous Titanic disaster. The task is to examine and predict what sorts of people were more likely to survive using data on their name, age, gender, socio-economic class, etc.\n",
    "\n",
    "#### 1. Loading our Data\n",
    "Load the train data into a pandas DataFrames. Display the first few rows **of each DataFrame** and print the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5jKqcYMkaQZ"
   },
   "outputs": [],
   "source": [
    "#Q1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7Ttum9PkaQa"
   },
   "source": [
    "#### 2. Checking for Null Values\n",
    "\n",
    "Check for null values in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiuRUqd4kaQa"
   },
   "outputs": [],
   "source": [
    "#Q1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snPzgZrJkaQb"
   },
   "source": [
    "#### 3. Checking the class balance\n",
    "Since this is a binary classification task, class balance is extremely important. What percent of the training data belongs to each class? Is this roughly balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wZr8wXtkaQb"
   },
   "outputs": [],
   "source": [
    "#Q1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7UeH0yBkaQb"
   },
   "source": [
    "You should see that the classes are roughly balanced, about a 60-40 split between not surviving and surviving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07Ij8E7KkaQb"
   },
   "source": [
    "# Q2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZVnuFfPkaQb"
   },
   "source": [
    "1. Fill missing values for Age and Fare with their respective median values, make sure to round age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hv66SOBRkaQb"
   },
   "outputs": [],
   "source": [
    "#Q2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPgOBRB5kaQb"
   },
   "source": [
    "2. Drop non-unique variables such as `PassengerId`, and irrelevant features such as `Ticket`, `Cabin`, and `Embarked`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YI4v4qdOkaQb"
   },
   "outputs": [],
   "source": [
    "#Q2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQrOELHDkaQb"
   },
   "source": [
    "3. Combine `SibSp` and `Parch` into a single variable `Family`. Then drop `SibSp` and `Parch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2w8dqkBkaQb"
   },
   "outputs": [],
   "source": [
    "#Q2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSpoPtqxkaQc"
   },
   "source": [
    "Like Assignment 2, the `Name` column contains passengers' full names, including their titles (e.g., Mr., Mrs., Dr., Col.). Write code to **extract only the title** from each passenger's name. Standardize similar titles (e.g., Ms --> Miss, Mlle --> Miss, Mme --> Mrs), and group rare or honorific titles (e.g., Capt, Col, Dr, Sir, etc.) under a single label called \"Special\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arCmmea6kaQc"
   },
   "outputs": [],
   "source": [
    "special_names = ['Capt','Col','Countess','Don','Dona','Dr','Jonkheer','Lady','Major','Rev','Sir']\n",
    "train['Name'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "train['Name'] = train['Name'].replace('Ms', 'Miss').replace('Mlle', 'Miss').replace('Mme', 'Mrs').replace(special_names, 'Special')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXV3qR4ikaQc"
   },
   "source": [
    "4. One-hot-encode the features `Name`, `Sex`, and `Pclass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iMfzTGAkaQc"
   },
   "outputs": [],
   "source": [
    "#Q2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hVLfz83kaQc"
   },
   "source": [
    "5. Correlation Map\n",
    "Visualize the correlation between variables using a heatmap. List the 3 features that are most related to a passenger's survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDQq_WOqkaQc"
   },
   "outputs": [],
   "source": [
    "#Q2.5 Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Epwuh8gkaQc"
   },
   "outputs": [],
   "source": [
    "#Q2.5 List of three features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Checkpoint 1](checkpoint_1.png)\n",
    "\n",
    "Your `train.head()` should look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-LR8MfZkaQc"
   },
   "source": [
    "# Q3 - Train/Test Split and Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRO8w-YTkaQc"
   },
   "source": [
    "#### 1. Scaling\n",
    "Scale the train. Make sure you do not scale the one-hot-encoded data or the target variable (Survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU_lcsYFkaQc"
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sugWyZnukaQc"
   },
   "outputs": [],
   "source": [
    "#Q3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the target from the training data and add it to its own dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['Survived']\n",
    "train.drop(['Survived'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSUw75dGkaQd"
   },
   "source": [
    "#### 2. Train/Test Split\n",
    "Split the training data into a train and test set using an 80-20 split. Make sure you preserve the balance of the target variable using the `stratify = target` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xClRJ-FqkaQd"
   },
   "outputs": [],
   "source": [
    "#Q3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV4Asd6TkaQd"
   },
   "source": [
    "#### 3. Check the class balance of the train and test set.\n",
    "Print the percent of samples that survived in both the train and test set. They should be almost equal since you stratified by the target variable when splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y64lgna6kaQd"
   },
   "outputs": [],
   "source": [
    "#Q3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MIRKxMTkaQd"
   },
   "source": [
    "# Q4 - Decision Tree Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yk-PBfR4kaQd"
   },
   "source": [
    "Decision trees have a number of parameters to adjust. For this assignment we have chosen `criterion`, `max_depth`, `max_features`, and `splitter` as the parameters to adjust. You will exhaustively search over some values of these parameters using a grid search.\n",
    "\n",
    "The grid and potential parameter values are defined below. For more information on what these parameters do, see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPv-XuTykaQd"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters for grid search\n",
    "param_grid = {'criterion' : ['gini', 'entropy'] # The function to measure the quality of a split.\n",
    "              , 'max_depth' : [1, 2, 3, 4, 5, None] # None means the tree can grow arbitrarily deep.\n",
    "              , 'max_features' : [2, 3, 4, 'sqrt', 'log2', None] # The number of features to consider when looking for the best split.\n",
    "              , 'splitter' : ['best', 'random'] # The strategy used to choose the split at each node.\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdRWvi82kaQd"
   },
   "source": [
    "#### 1. Instantiate a `DecisionTreeClassifier` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8T-9B1OkaQd"
   },
   "outputs": [],
   "source": [
    "#Q4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_vsAah8kaQh"
   },
   "source": [
    "#### 2. Conduct the grid search.\n",
    "\n",
    "Fit the grid search object on the training data and print the best cross-validation score and best parameters. Use the same parameters for the grid search as in lab 6 for results consistent with mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENnwwinqkaQh"
   },
   "outputs": [],
   "source": [
    "#Q4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXHWao13kaQh"
   },
   "source": [
    "#### 3. Evaluate on the validation set.\n",
    "Using the best parameters, evaluate the model on the validation set. Calculate the accuracy and AUC on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GouXCYj9kaQh"
   },
   "outputs": [],
   "source": [
    "#Q4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 2\n",
    "\n",
    "Your accuracy for this decision tree should be between ~70-83%. It it not controlled via a random seed so results will vary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EummW5tRkaQi"
   },
   "source": [
    "# Q5 - Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBBE8fj2kaQi"
   },
   "source": [
    "As we saw in lecture, a single decision tree can be unstable. We can use ensemble models to get more accurate predictions. You will train an XGBoost model which combines multiple trees to see if you can get a better performance. As with decision trees, there are a number of tunable parameters, so you will use a grid search to find the \"best\" ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVSjZ1ptkaQi"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],        # number of boosting rounds\n",
    "    'max_depth': [3, 4, 5, 6],              # depth of each tree\n",
    "    'learning_rate': [0.01, 0.05, 0.1],     # step size shrinkage\n",
    "    'subsample': [0.8, 1.0],                # fraction of samples used per tree\n",
    "    'colsample_bytree': [0.8, 1.0],         # fraction of features used per tree\n",
    "    'gamma': [0, 0.5, 1],                   # minimum loss reduction to make a split\n",
    "    'reg_lambda': [1, 5, 10]                # L2 regularization strength\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzPohNI0kaQi"
   },
   "source": [
    "#### 1. Instantiate an XGBoost model.\n",
    "\n",
    "Use the same parameters for the grid search as in lab 6 for results consistent with mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpijKGZ6kaQi"
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state = 42)\n",
    "#Q5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xurFyH6kaQi"
   },
   "source": [
    "#### 2. Conduct the grid search.\n",
    "\n",
    "Fit the grid search object on the training data and print the best cross-validation score and best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBl2aWo2kaQi"
   },
   "outputs": [],
   "source": [
    "#Q5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7BfSqF6kaQi"
   },
   "source": [
    "#### 3. Evaluate on the validation set.\n",
    "Using the best parameters, evaluate the model on the validation (test) set. Calculate the accuracy and AUC score on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZUUrS7xkaQi"
   },
   "outputs": [],
   "source": [
    "#Q5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 3\n",
    "\n",
    "Your XGboost model should be 83.24% accuracy with and AUC of 0.81. This is controlled by a random seed. If you do not get this value but are close, that is fine too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9apFQVDkaQi"
   },
   "source": [
    "# Q6 - Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2Yh4FcAkaQi"
   },
   "source": [
    "The last model you will train is a Neural Network. The Neural Network (NN) architecture will consist of 4 layers:\n",
    "1. Input (size = 8)\n",
    "2. Hidden1 (size = 16)\n",
    "3. Hidden2 (size = 16)\n",
    "4. Output layer (size = 2)\n",
    "\n",
    "This is a simple network that uses Relu activation function on each of the hidden layers, and a Sigmoid function on the output.\n",
    "\n",
    "You will train your model using Binary Cross Entropy Loss for 50 epochs, with an Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUKszpIckaQj"
   },
   "source": [
    "#### 1. Define the NN architecture.\n",
    "\n",
    "Complete the code below to define the NN architecture. You should have 4 layers, with activation functions between each layer.\n",
    "1. Input layer (linear)\n",
    "2. Apply ReLU\n",
    "3. First hidden layer (linear)\n",
    "4. Apply Relu\n",
    "5. Second hidden layer (linear)\n",
    "6. Apply Sigmoid\n",
    "\n",
    "Do not edit the `forward` function. The code you write should work without having to edit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQHJ5xQ4oxJc"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        #Q6.1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGWrD_cekaQj"
   },
   "source": [
    "#### 2. Prepare the data for training.\n",
    "\n",
    "Create tensor datasets from `X_train` and `y_train` and from `X_test` and `y_test`. To do this, first convert them to Pytorch tensors. Then turn these into DataLoaders with a batch size of 64. Make sure you shuffle the train loader but do not shuffle the test loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your data to tensors\n",
    "#Q6.2 Create tensor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnTO1rDfkaQj"
   },
   "outputs": [],
   "source": [
    "#Q6.2 Define Dataloaders, you can combine these two steps into a single cell if you'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZkPIoveqKHW"
   },
   "source": [
    "#### 4. Prepare for training\n",
    "\n",
    "1. Before instantiating your model, you will need to specify the size of each layer.\n",
    "2. Instantiate your model.\n",
    "3. Define your criterion, this if your loss function (remember we are doing binary classification).\n",
    "4. Declare your optimizer, use `optim.Adam` for this with a learning rate of `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTO0oGdiqJY1"
   },
   "outputs": [],
   "source": [
    "#Q6.4 1-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocBkC8MLkaQj"
   },
   "source": [
    "#### 5. Train your NN\n",
    "\n",
    "Use the training loop below to train your neural network. The loop keeps track of the training loss and accuracy so we can track their progress and plot them in the next question. There is no coding for you to do here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKVmIbpps4yS"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad() # Zero out the gradients\n",
    "        outputs = model(xb).squeeze(1) # Get the predictions\n",
    "        loss = criterion(outputs, yb) # Calculate the loss\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Optimize\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            outputs = model(xb).squeeze(1)\n",
    "            loss = criterion(outputs, yb)\n",
    "            val_loss += loss.item()\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "    val_loss /= len(test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    preds_binary = (torch.tensor(all_preds) > 0.5).float()\n",
    "    acc = accuracy_score(all_labels, preds_binary)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | AUC: {auc:.4f} | Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOB_kMIdy6g0"
   },
   "source": [
    "#### 6. Plot the training loss and accuracy\n",
    "\n",
    "Create a plot for the train and test (validation) loss over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3qN53dyy4b-"
   },
   "outputs": [],
   "source": [
    "#Q6.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0eIKsELtD_h"
   },
   "source": [
    "#### 7. Final Evaluation on the Test Set\n",
    "\n",
    "Use the code below to calculate the test accuracy and AUC score. Again, no code for you to add here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dttkMk0-kaQj"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "all_preds = []   # for AUC (raw sigmoid outputs)\n",
    "all_labels = []  # true labels\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        outputs = model(X_batch).squeeze()  # sigmoid probabilities\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # Store for AUC\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        # Accuracy (thresholded at 0.5)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        test_correct += (preds == y_batch).sum().item()\n",
    "        test_total += y_batch.size(0)\n",
    "\n",
    "# Compute averages\n",
    "avg_test_loss = test_loss / test_total\n",
    "acc_nn = test_correct / test_total\n",
    "\n",
    "# Compute AUC\n",
    "auc_nn = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"\\nTest Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {acc_nn:.4f}\")\n",
    "print(f\"Test AUC: {auc_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3Edx6rGkaQk"
   },
   "source": [
    "# Q7 - Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcC8DiJJkaQk"
   },
   "source": [
    "#### 1. Accuracy and AUC Comparison\n",
    "Display the accuracy and AUC of each model in a table using the `display()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApDeAPnZkaQk"
   },
   "outputs": [],
   "source": [
    "#Q7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "State which model performed best. If you wanted to attempt to improve performance of any of these models' performance what would you try and what is the reasoning behind your decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
